Usage
=============

# summarize

via_client_cli.py 에서 vss summarize 호출을 하고, 

via_server.py 에서 summarize 호출을 받아서 처리 (vlm 호출 등등)

## API call (cli)
```
via_client_cli.py summarize [-h] --id ID --model MODEL
   [--stream]
   [--chunk-duration CHUNK_DURATION]
   [--chunk-overlap-duration CHUNK_OVERLAP_DURATION]
   [--summary-duration SUMMARY_DURATION]
   [--prompt PROMPT]
   [--caption-summarization-prompt CAPTION_SUMMARIZATION_PROMPT]
   [--summary-aggregation-prompt SUMMARY_AGGREGATION_PROMPT]
   [--file-start-offset FILE_START_OFFSET]
   [--file-end-offset FILE_END_OFFSET]
   [--model-temperature MODEL_TEMPERATURE]
   [--model-top-p MODEL_TOP_P]
   [--model-top-k MODEL_TOP_K]
   [--model-max-tokens MODEL_MAX_TOKENS]
   [--model-seed MODEL_SEED]
   [--alert ALERT]
   [--enable-chat]
   [--response-format {json_object,text}]
   [--backend BACKEND]
   [--print-curl-command]
```
Note that --model-seed does not fix the final result

Sample:

```
python via_client_cli.py summarize --id c18b85f1-7865-48ab-a7ff-8184fe9c0180 --model gpt-4o --chunk-duration 2 --prompt "Write a dense caption about the video containing events like ..." --model-temperature 0.8 --backend http://10.0.35.
```

## via_client_cli.py 코드

함수: def do_summarize(args):

* vss 호출

api call 하는 line:

    response = requests.post(get_api_url("/summarize"), json=req_json, stream=args.stream)

* Return values

case 1. --stream

    for event in client.events():
        data = event.data.strip()
        result = json.loads(data)

case 2. not stream

    result = response.json()

## via_server.py 코드

ViaServer._stream_handler -> via_stream_handler.py 의 ViaStreamHandler 객체

ViaServer._asset_manager: 파일 관리 -> asset_manager.py의 AssetManager 객체

ViaServer.summarize() 내부에서 argument 설정, thread 생성하고, via_stream_handler.py 의 summarize() 호출 (line 1875) (주요 argument: prompt 및 id(나중에 id_list, assets 으로 명칭변화))

해당 request_id 으로 response 관리. wait 했다가 response 오면 get_response 거쳐서 바로 return


## via_stream_handler.py 코드

summarize() -> query() 순으로 호출

query() 안에서 argument재정렬 후 req_info 에 저장하고, _trigger_query() 호출

req_info 안에 많은 정보가 있지만, line2103 에서 return할때 client 단으로 보내는 정보는 적음. 이부분 수정하면 더많은 로깅 가능할듯.

_trigger_query() 안에서, req_info._ctx_mgr is not None by default 이고, from via_ctx_rag.context_manager import ContextManager 객체를 보유함. (CotextManager 객체는 비공개이고 rag관련 객체같음.) (또한 llm이 쓰이는 부분도 이부분같음? 아직확실치않음.)

_trigger_query() 안에서, enable_dense_caption is False by default

_trigger_query() 안에서, chunk 마다, _on_new_chunk 함수 안에서, line 822에서, vlm_pipeline.py의 _vlm_pipeline.enqueue_chunk 함수호출

## vlm_pipeline.py 코드

먼저 enqueue_chunk() 함수가 호출됨.

process_base.py 을 참고해야하고, _process() 함수가 호출되며, 여기서 vlm호출.

## vila15_model.py 코드

_get_input_ids_from_prompt 함수에서 image embedding 결합

self._tokenizer 변수가 tokenization

두 개 활용해서 image+prompt tokenization =⇒ line 339

그다음 VILA 모델의 일부인 LLM 파트로 입력됨 (self._model = AutoModelForCausalLM.from_pretrained( 에서 생성된 객체) 

여기 self._model.generate() 함수는 llava_arch.py의 generate() 으로 보임. (매핑 코드 동작로직을 쉽게 판단하기 힘들어서 추정으로 마무리)

## back to via_stream_handler.py코드

VLM response를 받아서, _on_vlm_chunk_response함수 내에서 self._process_output 호출함. ViaStreamHandler._get_aggregated_summary()함수 내부 코드에서 최종 답변 생성을 위해, VLM결과를 입력으로 (_ctx_mgr으로 관리하는) RAG 호출이 됨. (line 1850). 그리고, _ctx_mgr 내부적으로 데이터 저장. --> 밑에 다시한번 기록함.

* * *

# VIA Server `qa` Function Analysis

## Purpose

The `qa` function implements the `/chat/completions` API endpoint in the VIA server.  It provides interactive question-answering (Q&A) and chat capabilities on the input media (video files or live streams).  It allows users to ask questions about the content of the media and receive responses generated by the underlying language model, potentially augmented with information from a RAG (Retrieval-Augmented Generation) system.


##  Via_server.py

User의 메시지는 ChatCompletionQuery 객체로 관리되며, message 객체가 user prompt이며, history 누적되는 형식임, -1 위치가 가장 최신.

RAG-based Chat function 필수. (line 2238 에서 확인)

Line 2251에서 via_stream_handler.py의 ViaStreamHandler.qa() 함수 호출

Line 2264 코드는 return값을 보기좋게 만들어주는코드

##  Via_stream_handler.py

Line 950 에서 _ctx_mgr() 호출. Summarize때와 달리, ctr_mgr 에서 관리하는 이유는, video자체를 청킹하는 가공을 할 필요는 없으면서 (즉, vlm_pipeline.py 내부 모듈을 호출할 필요가 없음) query에 RAG + LLM을을 활용하고 관리할 필요가 있어서 그런 것 같음. 어차피 VLM결과는 DB에 저장되어있기 때문에 chat history 관리를 통해 통합 관리가 가능해보임.

##  context.manager.py 코드 내에서

Contextmanager.call 호출되고 바로이어서 ContextManagerProcess.call 호출되고, 바로 queue에 넣어짐.
그러면 ContextManagerProcess.run 함수에서 get 함. (line 103) 그리고 line 133 루트로 감. 그리고 ContextManagerHandler.call 함수 (line 470) 가 호출됨.

Line 477에서 self._functions[‘chat’] 함수 호출. 이 함수는 (graph rag이면) line 375 또는 (vector rag이면) line 397 에서 만들어지고 할당됨.

그 이후는 chat_function.py 와 graph_extraction_func.py & graph_retrieval_func.py 또는 chat_function.py 와 vector_extraction_func.py & vector_retrieval_func.py 봐야함. 여기서는 GraphExtractionFunc -> GraphRetrievalFunc 순서대로 호출됨 (또는 Graph->Vector)

이 다음 내부 동작은 neo4jDB 와 config 분석이 필요함.

## 기타:

Self._rails_config 객체는 via.guardrail_config 폴더를 참고하면 되며, 올바르지 않은 질문을 체크함.


##  Context_manager.py 에 관하여

VSS 구동 초기에 ContextManagerProcess 시작되면서 run함수까지 호출되는것으로 보이고, self._initialize() 함수 호출을 통해 세팅.

ContextManagerHandler.configure() 함수에서 Config 값 세팅, Db 세팅,  chat_llm = ChatOpenAITool() 세팅,

그리고 add_function 함수를 통해 summarization & chat 함수가 세팅됨. (line 312, 320, 372, 397) 그리고 self._functions 에 dictioanary형태로 함수가 관리되며, summarize / qa 시에 각각 필요한 함수 호출.

-> summarization 시엔 OfflineBatchSummarization / RefineSummarization

-> qa 시엔 ChatFunction / GraphExtractionFunc 등등

VectorRag 일때와 GraphRag일때 사용 DB 가 다른 것으로 보임. 각각 (MilvusDBTool / Neo4jGraphDB).

Via_server.py 및 via_stream_handler.py 코드에서, Summarize 시에는 ViaStreamHandler._get_aggregated_summary()함수 내부 코드에서 최종 답변 생성을 위해, VLM결과를 입력으로 (_ctx_mgr으로 관리하는) RAG 호출이 됨. (line 1850). 그리고, _ctx_mgr 내부적으로 데이터 저장. (_via_health_eval 경로는 로깅용). 그 이후context_manager.py 및 OfflineBatchSummarization 또는 RefineSummarization 내부에서 VLM 최종 결과 (RAG까지 활용한 최종 결과)가 DB 파일 형태로 저장되고,

그다음 qa시에 *_retrieval_func.py의 retrieve_documents() 및 *_extraction_func.py의 aprocess_doc 함수를 통해 history가 호출됨. 이 때 summarize 결과 또한 읽어짐. (즉, 두 번 읽어지는 것으로 보임) --> 이부분은 밑에 DB항목 참고.


# DB

## Interaction between summary and qa

_ctx_mgr.add_doc 함수 호출로 인해, context_manager.py의 line 104 (if item and "add_doc" in item:) 부분 루트를 탐. 그리고 Milvus/Neo4j DB에 기록됨.

그러면, _functions 가 관리하는 함수들, 즉 chat_function.py / offline_batch.py / refine.py 등의 aprocess_doc 함수가 연달아 호출됨. 

이 때, 

1. chat_function.py 의 extraction 함수쪽에선 aprocess_doc 함수를 부르고, vector_rag/graph_rag의 aprocess_doc 함수를 호출함. 
2. 또한 retrieval쪽에선 retrieve_documents()함수 호출함. 

### extraction / retrieve 역할 분석

TODO: 내용 채우기.



